Name: Nattapat Juthaprachakul, Student ID: 301350117

QUESTIONS:
In a text file answers.txt, answer these questions:

1.  What was wrong with the original wordcount-5 data set that made repartitioning worth it? Why did the program run faster after?
    ANS: Yes, when I run wordcount-improved.py with repartition on wordcount5 dataset, the time of running code did improve substantially. All of the code below is run on clusters. 

        1. No repartition: 7mins 29s (8 tasks) 
	2. Repartition 10 slices: 4mins 47s (10 tasks) 
        3. Repartition 15 slices: 3mins 24s (15 tasks)
         The program runs faster because the number of repartition controls amount of parallelism the program is running. 

2.  The same fix does not make this code run faster on the wordcount-3 data set. (It may be slightly slower?) Why? [For once, the answer is not “the data set is too small”.]
    How could you modify the wordcount-5 input so that the word count code can process it and get the same results as fast as possible? (It's possible to get about another minute off the running time.)
    ANS: dataset is wordcount3 and all code is run on clusters.
       1) No repartition: 31.504s,
       2) Repartition 10 slices: 37.947s,
       3) Repartition 15 slices: 28.402s,
       4) Repartition 20 slices: 43.981s
       There are 100 dataset files excluding README.txt in wordcount3 while there are only 8 dataset files in wordcount5.
       After observing the size of each dataset in wordcount3, the largest file is approximately 24 times larger than the smallest file and all of the file have very similar size data. While in wordcount5, the largest file is approximately 3,000 times larger than the smallest file and there 2 small files compared to several 100-1000 times in size of 6 files. Hence, this might be the reason that repartition helps improve performance on wordcount5 while does not do much to improve wordcount3 because the repartition needs a full shuffle. The shuffle requires moving the RDD's data around between workers which may potentially result in lots of network traffic. To increase efficiency in our wordcount-improved.py on wordcount5, we can do input preprocessing before running the code (balancing datasets and shrinking the data before shuffling).  

3.  When experimenting with the number of partitions while estimating Euler's constant, you likely didn't see much difference for a range of values, and chose the final value in your code somewhere in that range. What range of partitions numbers was “good” (on the desktop/laptop where you were testing)?
    ANS: Here is my record on running euler.py on Local machine (lab computer).
    1. With 10,000 samples, 
       1.1) No repartition: 30.476s, 
       1.2) Repartition 10 slices: 30.728s,
       1.3) Repartition 50 slices: 35.958s,
       1.4) Repartition 100 slices: 31.143s,
       1.5) Repartition 500 slices: 33.55s,
       1.6) Repartition 1,000 slices: 36.184s
    2. With 1,000,000 samples,
       2.1) No repartition: 30.849s,
       2.2) Repartition 5 slices: 30.760s,
       2.3) Repartition 10 slices: 30.484s,
       2.4) Repartition 50 slices: 31.018s,
       2.5) Repartition 100 slices: 30.785s,
       2.6) Repartition 110 slices: 30.910s,
       2.7) Repartition 200 slices: 32.143s,
       2.8) Repartition 250 slices: 33.499s,
       2.9) Repartition 500 slices: 34.043s,
       2.10) Repartition 1,000 slices: 36.853s,
       2.11) Repartition 1,500 slices: 40.585s
    3. With 1,000,000,000 samples,
       3.1) No repartition: 4mins 20s,
       3.2) Repartition 10,000: 4mins 46s,
       3.3) Reparition 1,000,000: takes more than 15mins.
    As we can see the results above that a number of partitions do depend on numbers of inputs. For 10,000 samples, the ranges are quite unclear. However, for 1,000,000 samples, I think the range of partitions are approximately between 0 to 100 inclusively as you can see the time the codes run between 30 and 31 seconds. 

4.  How much overhead does it seem like Spark adds to a job? How much speedup did PyPy get over the usual Python implementation? 
    ANS:
    1.) Local lab computer 1,000,000 samples 
      1.1 Standard CPython implementation (noraml spark that we run): 31.152s 
      1.2 Spark Python with PyPy (just-in-time compiler): 1 mins 16s 
    2.) Local lab computer 1,000,000,000 samples
      2.1 Standard CPython implementation (noraml spark that we run): 4 mins 21s
      2.2 Spark Python with PyPy (just-in-time compiler):  1 mins 58s
    For a very large sample (1,000,000,000 samples), the overhead from normal implementation(4mins 21s) is very obvious compared to just-in-time compiler (1min 58s).  
    
